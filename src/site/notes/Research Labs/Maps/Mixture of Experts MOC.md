---
{"dg-publish":true,"permalink":"/research-labs/maps/mixture-of-experts-moc/","tags":["mixture_of_experts"],"created":"2025-02-21T16:35:17.001+07:00","updated":"2025-03-09T22:31:56.086+07:00"}
---

## Paper

[[Research Labs/Notes/Mixture-of-Experts Meets Instruction Tuning - A Winning Combination for Large Language Models\|Mixture-of-Experts Meets Instruction Tuning - A Winning Combination for Large Language Models]]
[[Research Labs/Notes/ST-MoE - Designing Stable and Transferable Sparse Expert Models\|ST-MoE - Designing Stable and Transferable Sparse Expert Models]]
[[Research Labs/Notes/Switch Transformers - Scaling to Trillion Parameter Models with Simple and Efficient Sparsity\|Switch Transformers - Scaling to Trillion Parameter Models with Simple and Efficient Sparsity]]
[[Research Labs/Notes/GShard - Scaling Giant Models with Conditional Computation and Automatic Sharding\|GShard - Scaling Giant Models with Conditional Computation and Automatic Sharding]]
[[Research Labs/Notes/MomentumSMOE - Integrating Momentum into Sparse Mixture of Experts\|MomentumSMOE - Integrating Momentum into Sparse Mixture of Experts]]
[[Research Labs/Notes/Outrageously Large Neural Networks - The Sparsely-Gated Mixture-of-Experts Layer\|Outrageously Large Neural Networks - The Sparsely-Gated Mixture-of-Experts Layer]]













