---
{"up":["[[Atlas]]"],"related":null,"created":"2025-02-19 22:33:26","tags":["mixture_of_experts"],"dg-publish":true,"permalink":"/research-labs/maps/mixture-of-experts-moc/","dgPassFrontmatter":true}
---

## Paper

[[Research Labs/Notes/Mixture-of-Experts Meets Instruction Tuning - A Winning Combination for Large Language Models\|Mixture-of-Experts Meets Instruction Tuning - A Winning Combination for Large Language Models]]
[[Research Labs/Notes/ST-MoE - Designing Stable and Transferable Sparse Expert Models\|ST-MoE - Designing Stable and Transferable Sparse Expert Models]]
[[Research Labs/Notes/Switch Transformers - Scaling to Trillion Parameter Models with Simple and Efficient Sparsity\|Switch Transformers - Scaling to Trillion Parameter Models with Simple and Efficient Sparsity]]
[[Research Labs/Notes/GShard - Scaling Giant Models with Conditional Computation and Automatic Sharding\|GShard - Scaling Giant Models with Conditional Computation and Automatic Sharding]]
[[Research Labs/Notes/MomentumSMOE - Integrating Momentum into Sparse Mixture of Experts\|MomentumSMOE - Integrating Momentum into Sparse Mixture of Experts]]
[[Research Labs/Notes/Outrageously Large Neural Networks - The Sparsely-Gated Mixture-of-Experts Layer\|Outrageously Large Neural Networks - The Sparsely-Gated Mixture-of-Experts Layer]]













