---
{"dg-publish":true,"permalink":"/research-labs/maps/mixture-of-experts-moc/","tags":["mixture_of_experts"]}
---

## Paper

[Mixture-of-Experts Meets Instruction Tuning - A Winning Combination for Large Language Models](Mixture-of-Experts%20Meets%20Instruction%20Tuning%20-%20A%20Winning%20Combination%20for%20Large%20Language%20Models.md)
[ST-MoE - Designing Stable and Transferable Sparse Expert Models](ST-MoE%20-%20Designing%20Stable%20and%20Transferable%20Sparse%20Expert%20Models.md)
[Switch Transformers - Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](Switch%20Transformers%20-%20Scaling%20to%20Trillion%20Parameter%20Models%20with%20Simple%20and%20Efficient%20Sparsity.md)
[GShard - Scaling Giant Models with Conditional Computation and Automatic Sharding](GShard%20-%20Scaling%20Giant%20Models%20with%20Conditional%20Computation%20and%20Automatic%20Sharding.md)
[MomentumSMOE - Integrating Momentum into Sparse Mixture of Experts](MomentumSMOE%20-%20Integrating%20Momentum%20into%20Sparse%20Mixture%20of%20Experts.md)
[Outrageously Large Neural Networks - The Sparsely-Gated Mixture-of-Experts Layer](Outrageously%20Large%20Neural%20Networks%20-%20The%20Sparsely-Gated%20Mixture-of-Experts%20Layer.md)













