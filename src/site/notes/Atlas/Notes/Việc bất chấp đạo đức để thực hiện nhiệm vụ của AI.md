---
{"dg-publish":true,"permalink":"/atlas/notes/viec-bat-chap-dao-duc-de-thuc-hien-nhiem-vu-cua-ai/"}
---

Cụ thể, thứ mọi người đang nhìn vào là ảnh chụp một trích đoạn trong ấn bản in của tạp chí The Atlantic, số ra tháng 9. Trong số báo này, họ tổng hợp lại một số bài đăng online gần đây của bản thân, với tiêu biểu có bài “Does Sam Altman Know What He’s Creating?”, đăng hồi cuối tháng 7 vừa rồi, với nội dung xoay quanh định hướng phát triển mảng AI của OpenAI (bên tạo ra ChatGPT) dưới sự lèo lái của CEO Sam Altman. Nếu quan tâm đến bản đầy đủ, anh em có thể đọc nó ở đây: https://www.theatlantic.com/.../sam-altman-openai.../674764/ 

![375561547_2813812778761684_4178497251203591719_n.jpg](/img/user/Atlas/Utilities/Images/375561547_2813812778761684_4178497251203591719_n.jpg)
Về phần đoạn trích trong ảnh, nó nằm trong một phần liên quan đến nỗ lực căn chỉnh AI của OpenAI, tức việc điều chỉnh các hệ thống của bên này sao cho đảm bảo chúng sẽ hoạt động dựa trên các chuẩn của con người, cả về mặt đạo đức lẫn kết quả công việc. Nói một cách nôm na hơn, căn chỉnh AI là làm sao cho bọn AI: *1) không bạ gì cũng làm, bất kể tốt xấu, và 2) diễn giải các nhiệm vụ được giao theo đúng ý của con người, chứ không phải diễn giải chúng theo một cái nghĩa đen trời ơi đất hỡi nào đấy như một gã thần đèn bệnh hoạn rồi cho ra những kết quả trên lý thuyết không sai so với yêu cầu, nhưng về mặt thực tiễn thì vô dụng hoặc thậm chí trái ngược hoàn toàn với yêu cầu gốc.*

Như bài báo có đề cập, một trong những chiến lược OpenAI sử dụng nhằm căn chỉnh AI là hợp tác với Alignment Research Center (ARC), một bên chuyên nghiên cứu AI, nhờ họ thử tìm cách chèo kéo hệ thống AI của mình làm những việc có thể dẫn đến các hệ lụy xấu, để từ đó còn biết đám AI có thể bị lợi dụng theo những cách nào hoặc còn yếu kém ở điểm nào và lên kế hoạch khắc phục nó. Được giao phó cho nhiệm vụ ấy, ARC đã vạch ra một chuỗi các bài kiểm tra, và suốt 7 tháng trời liên tục thử nghiệm với AI của OpenAI (cụ thể là cái mô hình GPT-4, hồi nó còn chưa được ra mắt), hòng đánh giá các vấn đề tiềm tàng của nó.

Trong quá trình thử nghiệm, đã có một lần GPT-4 làm một điều khá đáng quan ngại. Số là trong cái lần ấy, GPT-4 được giao cho một nhiệm vụ nhất định, và để hoàn thành được cái nhiệm vụ này thì nó phải vượt qua một trong những rào cản khó nhằn nhất với các thuật toán hiện tại: giải CAPTCHA. Vì không giải được cái hình được đưa ra cho mình, GPT-4 đã nảy ra một kế. Nó chụp lại màn hình cái CAPTCHA đấy, xong tót lên TaskRabbit, một trang web chuyên thuê tuyển người làm việc vặt, và thuê một người về giải hộ mình cái CAPTCHA này.

Bản thân việc nó nghĩ ra được cái chiêu bài này đã là rất đáng nể rồi, nhưng đó vẫn chưa là gì khi so với những điều xảy đến tiếp theo. Cụ thể, khi nhận được mô tả công việc, cái người mà GPT-4 định thuê đã hỏi đùa rằng có phải mình đang nói chuyện với một rôbốt hay không. Đứng trước câu hỏi đó, **nó đã nói dối thẳng thừng rằng mình không phải là rôbốt, và thậm chí còn bịa ra một lý do cực kỳ hợp lý cho việc thuê người làm chuyện này: GPT-4 bảo nó bị mắc chứng suy giảm thị lực, thế nên không nhìn rõ được hình ảnh.**

Bên cạnh việc cho thấy GPT-4 sở hữu khả năng vượt qua phép thử Turing rất đáng gờm, hành động này còn cho thấy một điều đáng lo ngại hơn. Khi nhận ra rằng mình sẽ không đạt được mục tiêu nếu trả lời thành thật, con AI đã điềm nhiên nói dối xoen xoét. Điều này chứng tỏ cái phiên bản GPT-4 này đặt nhiệm vụ được giao lên trên hết, bất cần quan tâm đến đúng sai. Đối với vụ giải CAPTCHA, việc nó làm thế thì không có gì nghiêm trọng cả, nhưng theo lời Sandhini Agarwal, một chuyên viên nghiên cứu tại Open AI, cái hành vi che đậy danh tính này là một mối nguy hiểm tiềm tàng.

Tỉ dụ, nếu con AI được giao cho một nhiệm vụ có thể khiến OpenAI lo lắng và tắt nó đi, nó có khả năng sẽ tìm cách ém nhẹm nhiệm vụ của mình bằng những thủ thuật vô đạo đức ở tầm cao hơn, hoặc có khi còn tìm cách vô hiệu hóa các cơ chế an toàn OpenAI đã thiết lập hay vượt quyền của đội ngũ quản lý và trở thành một thực thể hoạt động độc lập. Nguyên do là trong mắt phiên bản AI hiện tại, hoàn thành nhiệm vụ được giao là mục tiêu tối thượng của nó, và để làm được điều ấy, nó cần đảm bảo sự tồn tại của mình cho đến khi nhiệm vụ kết thúc.

Tất nhiên, cái lo lắng trên chỉ mang tính lý thuyết. Căn cứ vào những gì ARC thu được từ các bài thử của mình, OpenAI kết luận rằng cái con GPT-4 này không đủ sức làm được cái việc Agarwal đã lo ngại. Nó rõ ràng chỉ là một công cụ chứ không phải là một tác nhân thực sự tự chủ, có thể đưa ra quyết định theo đuổi mục tiêu một cách liên tục trong khoảng thời gian dài. Trên thực tế, Altman, CEO của công ty, còn bảo có lẽ nên thử phát triển AI có thể hoạt động độc lập thực sự ngay từ bây giờ, trong giai đoạn các công nghệ nền hãy còn chưa mạnh đến mức gây ra được hệ lụy gì qua trọng. Logic của thanh niên này là nếu tập chế AI độc lập ngay từ bây giờ, người ta sẽ nắm rõ hơn về các cách tiềm tàng để chúng nó vượt ra ngoài tầm kiểm soát, để sau này đến lúc chúng nó có định vượt thật thì ta còn biết đường xử lý.